<head>
	<link rel="stylesheet" type="text/css" href="../mystyle.css">
	<link rel="icon" href="../site_images/siteidentity.png">

<b> Previous:</b> <a href="../ml_blog/logistic_regression2.html">Logistic Regression-2</a><br>
<b> Next:</b> <a href="../ml_blog/regularization.html">Regularization<br></a>

<br>

</head>
<body>
	<br> 
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	<img src="../site_images/logo.png" class="logo">
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

		<p class="align2"><font face="cinzel" size="4">
			<a href="https://beyondwhy.site">HOME</a>&nbsp;&nbsp;&nbsp;&nbsp;
			<a href="../pages/blog.html">BLOG</a>&nbsp;&nbsp;&nbsp;&nbsp;
			<a href="../pages/ebooks.html">EBOOKS</a>&nbsp;&nbsp;&nbsp;&nbsp;
			<a href="../pages/about.html">ABOUT</a>&nbsp;&nbsp;&nbsp;&nbsp;
			<a href="../pages/contact.html">CONTACT</a>&nbsp;&nbsp;&nbsp;&nbsp;
			<a href="../pages/shop.html">SHOP</a>
			</font></p>

<br>&nbsp;
<br>&nbsp;

<p>
<font size="-1"></font>
</p><div align="CENTER"><font size="-1"><font size="+1"><small><big><font size="+5"><b>Logistic Regression Example</b></p></font>
<br>
<br>&nbsp;
<br>&nbsp;
<hr style="width:70%">

</big></small></font></font></div><font size="-1"></font>
<p>
</p>
<a name="CHILD_LINKS"></a>

<br>&nbsp;
<br>&nbsp;
<font size="4" >


	<p class="marg">In this section, we will try to implement a very simple example of logisitic regression.</p>

	<p class="marg">For this example, we will try to predict the result(pass/fail) of a student based on his 2 test scores. The data set we are using can be downloaded here: <a href="https://beyondwhy.site/scoresvspass.txt">scoresvspass</a>.</p><br>

	<p class="p2">>> data = load('scoresvspass.txt') <span class="p4"> #load the data text file</p>
	<p class="p2">>> x = data(:,[1,2]); <span class="p4"> #store the 2 test scores as x</p>
	<p class="p2">>> y = data(:,3); <span class="p4"> #store the results as y</p>


	<p class="p2">>> global m = length(x); <span class="p4"> #store the no of training examples as m</p><br>

	<p class="p4"> #it is declared as global so that it can be accessed from within a function</p>
	<p class="p4"> #For faster implementation, this data can be feature scaled as follows.</p>
	<br>

	<p class="p2">>> global avgx = sum(x)./m; <span class="p4"> #average value in each column of x</p>
	<p class="p2">>> global rangex = max(x)- min(x); <span class="p4"> #maximum - minimum value in each column</p>
	<p class="p2">>> x = (x-avgx)./rangex; <span class="p4"> </p>
	<p class="p2">>> plot(x(pos, 1), x(pos, 2),
	 'k+','LineWidth', 2, 'MarkerSize', 10); <span class="p4"> </p>
	<p class="p2">>> hold on; <span class="p4"></p>
	<p class="p2">>> plot(x(neg, 1), x(neg, 2),
	 'ko', 'MarkerFaceColor', 'y','MarkerSize', 10); <span class="p4"></p>
	<p class="p2">>> hold on; <span class="p4"></p>
	<p class="p2">>> xlabel('Score 1'); <span class="p4"></p>
	<p class="p2">>> ylabel('Score 2'); <span class="p4"></p>

	<p class="p2">>> legend('Pass', 'Fail'); <span class="p4"></p>

	<br>
	<p><img src="../ml_images/logresex1.png" width="560px" border="1" class="bigimg"></p>
	<br>

	<p class="p2">>> X = [ones(m, 1) x]; <span class="p4"> #matrix X includes the x0 values</p>
	<br>


	<p class="p2">>> function J = findCost(X,y,theta)<span class="p4"> #declare the cost function</p>
	<p class="p2">J = 0; #initialize cost as zero</p>
	<p class="p2">global m;</p>
	<p class="p2">z = X* theta;</p>
	<p class="p2">htheta = 1 ./ (1 + exp(-z));<span class="p4"> #Hypothesis function sigmoid of X*theta</p>
	<p class="p2">J = sum((-y.*log(htheta)) - 
		(1-y).*log(1-(htheta)))/m;<span class="p4">  #Cost value J</p>
	<p class="p2">end;</p><br>


	<p class="p2">>> global theta = zeros(3, 1);<span class="p4"> #initialize theta as 0,0 or any other values</p>
	<p class="p2">>> iterations = 15000;<span class="p4"> #a random large no</p>
	<p class="p2">>> alpha = 0.09;<span class="p4"> #learning rate</p>
	<p class="p2">>> J_values = zeros(iterations, 1);<span class="p4"> #Initialize the cost values before iteration</p><br>

	<p class="p2">>> function [theta,J_values] = gradientDescent(X, y, theta, alpha, num_iters)<span class="p4"> #Declare the gradient descent function</p>
	<p class="p2">for i = 1:num_iters<span class="p4"> #Loop to iterate till cost function minimizes</p>
	<p class="p2">global m;</p>
	<p class="p2">z = X* theta;</p>
	<p class="p2">htheta = 1 ./ (1 + exp(-z));</p>
	<p class="p2">error = htheta -y;</p>
	<p class="p2">theta = theta - ((alpha/m) * (X'*error));<span class="p4"> #gradient descent vector implementation</p>
	<p class="p2">J_values(i) = findCost(X, y, theta);<span class="p4"> #Cost value at an iteration</p>
	<p class="p2">end;</p>
	<p class="p2">end;</p><br>

	<p class="p2">>> [theta,J_values] = gradientDescent(X, y, theta, alpha, iterations);<span class="p4"> #calling the gradient descent function</p>
	<p class="p2">>> theta<span class="p4"> #theta values corresponding to minimum cost</p>


	<p class="p3">theta = <br>
	&nbsp;&nbsp;&nbsp;&nbsp;0.82597<br>
	&nbsp;&nbsp;&nbsp;&nbsp;10.33155<br>
	&nbsp;&nbsp;&nbsp;&nbsp;9.92384</p><br>
	
	<p class="p2">>> x1= [min(X(:,2)),max(X(:,2))];<span class="p4"> #x axis of decision boundary</p>
	<p class="p2">>> y1 = (-1./theta(3)).*(theta(2).*x1 + theta(1)); </p>

	<p class="p4">#equation of decision boundary: theta1 + (theta2)x + (theta3)y = 0</p>
	<p class="p4">#rearranging to express y in terms of x</p>

	<p class="p2">>> plot(x1,y1);</p>
	<p class="p2">>> legend('Pass', 'Fail', 'Decision boundary');</p><br>

	<br>
	<p><img src="../ml_images/logresex2.png" width="560px" border="1" class="bigimg"></p>
	<br>


	<p class="p2">>> function prediction = predict(score1,score2)<span class="p4"> #declare the predict function</p>
	<p class="p2">global avgx;</p>
	<p class="p2">global rangex;</p>
	<p class="p2">global theta;</p>
	<p class="p2">prediction = 0;</p>
	<p class="p2">scores =([score1,score2] - avgx)./rangex;<span class="p4"> #scaling the input scores</p>
	<p class="p2">prediction = [1, scores]*theta;</p>
	<p class="p2">prediction = 1 ./ (1 + exp(-prediction));<span class="p4"> #sigmoid</p>
	<p class="p2">end;</p><br>

	<p class="p2">>> prediction = predict(60,75)<span class="p4"> #Predicting the result of a student with test scores 60 and 75</p>
	<p class="p3">prediction = 0.87259</p><br>
	 

	<p class="marg">Notice that the predicted result is a value between 0 and 1, it can be interpreted as the probability that the result is a positive (y=1). In our example, for test scores 60 and 75, there is a 87% probability that the student passes.</p>

	<p class="marg">An additional condition like the following can be used inside the predict function for binary output.</p>

	<p class="p2">>> if prediction >= 0.5</p>
	<p class="p2">output = 1;</p>
	<p class="p2">else</p>
	<p class="p2">output = 0;</p>
	<p class="p2">end;</p>
	
	
<br>
<br>

</body>
</html>
